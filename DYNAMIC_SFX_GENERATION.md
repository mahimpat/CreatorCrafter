# Dynamic SFX Generation - No Static Mappings!

## The Problem You Identified ‚úÖ

**Old approach had static mappings:**
```python
action_to_sfx = {
    'person walking': 'footsteps on ground',
    'person running': 'running footsteps',
    # ... hardcoded dictionary
}
```

**This was limiting because:**
- Can only suggest SFX for predefined actions
- No creativity or context awareness
- Couldn't handle novel situations
- Required manual updates for new scenarios

## New Dynamic Approach üöÄ

### 1. Vision-Language Model (BLIP)

**What it does:**
- Looks at each frame and generates natural language descriptions
- NOT predefined categories - free-form understanding
- Generates THREE descriptions per frame:
  1. **General**: "a man walking down a street carrying a bag"
  2. **Action-focused**: "a person is walking"
  3. **Sound-focused**: "footsteps, traffic sounds, urban ambience"

**Code** (lines 83-133):
```python
# No predefined categories!
# Model freely describes what it sees
general_description = model.generate(image)
# Model describes sounds it would expect
sound_description = model.generate(image, "What sounds would you hear?")
```

### 2. Dynamic Sound Conversion

**How it works** (lines 201-281):
```python
def convert_visual_to_audio_description(visual_desc, action_desc, sound_desc):
    # PRIORITY 1: Use model's direct sound description
    if sound_desc:
        return sound_desc  # e.g., "footsteps, traffic, urban ambience"

    # PRIORITY 2: Intelligently infer from visual description
    # Looks for keywords in natural language, not fixed categories
    combined = f"{visual_desc} {action_desc}"
    # e.g., "a man walking down a street carrying a bag"

    # Flexible pattern matching on ANY description
    if 'walking' in combined:
        add_sound('footsteps')
    if 'car' in combined:
        add_sound('car engine, vehicle sounds')

    # FALLBACK: Generate from description
    return f"ambient sounds for {visual_desc}"
```

### 3. What Makes It Dynamic?

**‚úÖ Vision Model generates free-form descriptions**
- Not limited to predefined categories
- Can describe ANY scene: "sunset over mountains", "busy restaurant", "person juggling"

**‚úÖ Sound descriptions from the model itself**
- BLIP can answer "What sounds would you hear?"
- Direct audio understanding from visual content

**‚úÖ Flexible keyword extraction**
- Analyzes natural language descriptions
- Finds sound-relevant elements dynamically
- Combines multiple detected sounds

**‚úÖ Context-aware fallbacks**
- If no patterns match, generates: "ambient sounds for {whatever_the_model_saw}"
- AudioCraft can interpret these natural descriptions

### 4. Example Outputs

**Scenario: Video of a person cooking**

**Static Approach (OLD):**
```json
{
  "prompt": "kitchen sounds, utensils",
  "reason": "Detected: kitchen"
}
```

**Dynamic Approach (NEW):**
```json
{
  "description": "a woman stirring a pot on a stove in a modern kitchen",
  "action_description": "a person is cooking",
  "sound_description": "sizzling, stirring sounds, kitchen ambience",
  "prompt": "sizzling, stirring sounds, kitchen ambience",
  "reason": "Scene: a woman stirring a pot on a stove..."
}
```

**Scenario: Video of unusual action (juggling fire)**

**Static Approach (OLD):**
```json
{
  "prompt": "smooth transition swoosh",
  "reason": "Detected scene change"
}
```
*Completely misses the context!*

**Dynamic Approach (NEW):**
```json
{
  "description": "a person juggling flaming torches outdoors at night",
  "action_description": "a person is performing",
  "sound_description": "fire crackling, whooshing air, outdoor ambience",
  "prompt": "fire crackling, whooshing air, outdoor ambience",
  "reason": "Scene: a person juggling flaming torches..."
}
```

### 5. Dialogue-Based Dynamic Detection

**Lines 320-384** - No hardcoded phrase combinations!

**Old approach:**
```python
if 'knock' in text and 'door' in text:
    sfx = 'knocking on wooden door'
```

**New approach:**
```python
# Detects ANY sound word mentioned
sound_words = ['knock', 'bang', 'crash', 'splash', 'ring', ...]

# PLUS dynamic object-action detection
words = text.split()
for obj, action in word_pairs:
    sfx_prompt = f"{obj} {action} sound"
    # e.g., "glass breaking sound", "balloon popping sound"
```

Generates SFX for ANY object-action combination mentioned, not just predefined ones!

### 6. How It Compares

| Feature | Static Mapping | Dynamic VLM |
|---------|---------------|-------------|
| Scene understanding | Predefined categories | Natural language |
| SFX suggestions | Fixed dictionary | Generated from vision |
| Handles novel scenes | ‚ùå No | ‚úÖ Yes |
| Sound descriptions | Hardcoded | Model-generated |
| Creativity | Limited | High |
| Requires updates | Yes (manual) | No (adaptive) |

## Making It Even More Dynamic üîÆ

### Current Limitations

The `convert_visual_to_audio_description` function still uses pattern matching as a fallback.

### Future Enhancement: LLM-Based Generation

Add a small language model to generate SFX prompts:

```python
def convert_visual_to_audio_description(visual_desc, action_desc, sound_desc):
    # Use small LLM (Phi-2, TinyLlama, etc.)
    prompt = f"""
    Visual scene: {visual_desc}
    Action: {action_desc}

    Generate a descriptive audio prompt for sound effects that would match this scene.
    Focus on specific sounds, ambience, and environment.

    Audio prompt:"""

    sfx_prompt = llm.generate(prompt, max_tokens=50)
    return sfx_prompt
```

This would be 100% dynamic with zero hardcoded mappings!

### Installation for Full Dynamic Mode

To add LLM-based generation:

```bash
pip install transformers accelerate

# Choose a small model:
# - microsoft/phi-2 (2.7B params)
# - TinyLlama/TinyLlama-1.1B-Chat-v1.0
# - facebook/opt-1.3b
```

## Performance

**Current Implementation:**
- BLIP model: ~500MB download (one-time)
- Processing: ~1-2s per frame on CPU, ~0.3s on GPU
- Memory: ~2GB RAM during analysis

**With LLM Addition:**
- Additional ~2-5GB for small LLM
- +0.5s per SFX generation
- Fully dynamic, zero hardcoding

## Testing

Test the dynamic analysis:

```bash
source venv/bin/activate

# Analyze a video
python python/video_analyzer.py \
    --video your_video.mp4 \
    --audio extracted_audio.wav \
    | jq '.suggestedSFX'
```

You'll see outputs like:
```json
{
  "timestamp": 5.2,
  "prompt": "footsteps, street traffic, urban sounds",
  "reason": "Scene: a man walking down a busy city street",
  "visual_context": "a man walking down a busy city street with cars",
  "action_context": "a person is walking",
  "confidence": 0.85
}
```

## Key Advantages

1. **No Predefined Categories**: Vision model describes scenes freely
2. **Model-Generated Sound Descriptions**: BLIP directly suggests sounds
3. **Flexible Pattern Matching**: Works with any natural language description
4. **Contextual Fallbacks**: Even unknown scenes get appropriate SFX
5. **Handles Novel Situations**: Can analyze ANY video content
6. **Self-Improving**: Better models = better suggestions, no code changes

## Summary

**Before:** Static dictionary lookups
**Now:** Vision-language model generates dynamic descriptions
**Future:** Add LLM for 100% generated SFX prompts

The system is now **context-aware** and **adaptive** rather than rule-based!
